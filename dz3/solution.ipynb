{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00580939-ace1-47bb-a439-094bcbaeb82d",
   "metadata": {},
   "source": [
    "### 1. Initialize spark\n",
    "\n",
    "- Run this cell to initialize spark session and install required packages for communication with EventHub. \n",
    "- This cell needs to executed once in the beginning to establish the SparkSession, you can later run other cells without re-executing this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eae046-c99c-4edc-b4f8-fda2628df7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"py4j\").setLevel(logging.CRITICAL)\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "scala_version = '2.12' \n",
    "\n",
    "print(\"Spark version\", pyspark.__version__)\n",
    "\n",
    "packages = [\n",
    "    f'com.microsoft.azure:azure-eventhubs-spark_{scala_version}:2.3.18'\n",
    "]\n",
    "\n",
    "args = os.environ.get('PYSPARK_SUBMIT_ARGS', '')\n",
    "if not args:\n",
    "    args = f'--packages {\",\".join(packages)}'\n",
    "    print('Using packages', packages)\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = f'{args} pyspark-shell'\n",
    "else:\n",
    "    print(f'Found existing args: {args}') \n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "   .master(\"local\")\\\n",
    "   .appName(\"eventhub-example\")\\\n",
    "   .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 1)\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"Europe/Zagreb\")\n",
    "spark.conf.set(\"spark.sql.streaming.checkpointLocation\", \"/tmp/pmf/lab3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140dc484-34aa-4f8f-b901-bff6ee66676c",
   "metadata": {},
   "source": [
    "### 2. Set Eventhub parameters & model of the incoming telemetry messages\n",
    "\n",
    "Eventhub configuration and JSON structure of the telemetry messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea69ac7-ef37-4c4c-a47a-dcbda7ee23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructField, StructType, DoubleType, TimestampType\n",
    "\n",
    "# Configure the connection string for your Eventhub\n",
    "EVENTHUB_CONNECTION_STRING = \"\"\n",
    "\n",
    "# Define Eventhub parameters configurations\n",
    "eventhubs_reader_conf = {\n",
    "    \"eventhubs.connectionString\" : spark._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(f\"{EVENTHUB_CONNECTION_STRING};EntityPath=telemetry\"),\n",
    "}\n",
    "eventhubs_publisher_conf = {\n",
    "    \"eventhubs.connectionString\" :  spark._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(f\"{EVENTHUB_CONNECTION_STRING};EntityPath=aggregates\"),\n",
    "}\n",
    "# JSON schema of incoming telemetry message\n",
    "json_schema = StructType([ \\\n",
    "    StructField('timestamp', TimestampType(), True), \\\n",
    "    StructField('vehicle_id', StringType(), True), \\\n",
    "    StructField('engine_speed', DoubleType(), True), \\\n",
    "    StructField('vehicle_speed', DoubleType(), True), \\\n",
    "    StructField('odometer', DoubleType(), True), \\\n",
    "    StructField('latitude', DoubleType(), True), \\\n",
    "    StructField('longitude', DoubleType(), True), \\\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e68a4f9-0a68-472b-909f-4cf14702a0e8",
   "metadata": {},
   "source": [
    "### 3. Example: Reading from Eventhub and displaying telemetry messages in the console output\n",
    "\n",
    "- check your console output where you started the docker command to create jupyter container, you should see an empty table output\n",
    "- now it's time to start your publisher.py - the messages will start to appear in the console output as they are read by your notebook script\n",
    "- output format displayed in the console is as follows:  **[timestamp, vehicle_id, engine_speed, vehicle_speed, odometer, latitude, longitude]**\n",
    "\n",
    "Stop the script by stopping the execution of this cell (Stop button in the toolbar)\n",
    "- When stopping the cell with stream query KeyboardInterrupt Error log is shown, but that is expected due to the internal logic of how jupyter works with Spark Structured Streaming\n",
    "- In case you mistakenly don't stop the cell with active stream/query and run a new one, please restart the jupyter kernel to avoid painfull debugging :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f46d3-f658-4853-9f37-bc33b944eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_and_await_termination(query):\n",
    "    try:\n",
    "        query.awaitTermination()\n",
    "    except KeyboardInterrupt:\n",
    "        query.stop()\n",
    "\n",
    "# Read from Eventhub stream\n",
    "df = spark.readStream.format(\"eventhubs\").options(**eventhubs_reader_conf).load()\n",
    "\n",
    "# Decode the binary messages contained in \"value\" column\n",
    "df = df.withColumn(\"body\", col(\"body\").cast(\"string\"))\n",
    "\n",
    "# Parse the JSON structure using the schema\n",
    "df = df.select(from_json(col(\"body\").cast(\"string\"), json_schema).alias(\"body\")).select(\"body.*\")\n",
    "\n",
    "# Output messages to console output (where the docker is running)\n",
    "query = df.writeStream.outputMode(\"append\").format(\"console\").option(\"truncate\", False).start()\n",
    "\n",
    "start_and_await_termination(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11878bf-a308-470a-bf6a-b145489185a2",
   "metadata": {},
   "source": [
    "### 4. Solving the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f96b34-8b7b-418f-98bc-b7db5fdd6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, lit, to_json, struct, max as pyspark_max, min as pyspark_min, mean as pyspark_mean\n",
    "\n",
    "def start_and_await_termination(query):\n",
    "    try:\n",
    "        query.awaitTermination()\n",
    "    except KeyboardInterrupt:\n",
    "        query.stop()\n",
    "\n",
    "def aggregate_data(df, aggregation_function, aggregation_name):\n",
    "    return df.withWatermark(\"timestamp\", \"5 seconds\") \\\n",
    "             .groupby(\"vehicle_id\", window(\"timestamp\", \"5 seconds\")) \\\n",
    "             .agg(aggregation_function(\"vehicle_speed\").alias(\"value\")) \\\n",
    "             .select(\n",
    "                 \"vehicle_id\",\n",
    "                 lit(aggregation_name).alias(\"type\"),\n",
    "                 lit(\"vehicle_speed\").alias(\"channel\"),\n",
    "                 col(\"value\"),\n",
    "                 col(\"window.start\").alias(\"window_start\"),\n",
    "                 col(\"window.end\").alias(\"window_end\")\n",
    "             )\n",
    "\n",
    "\n",
    "df = spark.readStream.format(\"eventhubs\").options(**eventhubs_reader_conf).load()\n",
    "df = df.withColumn(\"body\", col(\"body\").cast(\"string\"))\n",
    "df = df.select(from_json(col(\"body\").cast(\"string\"), json_schema).alias(\"body\")).select(\"body.*\")\n",
    "\n",
    "output_max = aggregate_data(df, pyspark_max, \"max\")\n",
    "output_min = aggregate_data(df, pyspark_min, \"min\")\n",
    "output_avg = aggregate_data(df, pyspark_mean, \"avg\")\n",
    "\n",
    "output = output_max.union(output_min).union(output_avg)\n",
    "output_json = output.select(to_json(struct(\"*\")).alias(\"body\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f9a85-1943-416c-b6c6-81b36d23ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = output_json.writeStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**eventhubs_publisher_conf) \\\n",
    "    .start()\n",
    "\n",
    "start_and_await_termination(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
